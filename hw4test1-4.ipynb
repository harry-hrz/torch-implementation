{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Dict, List, Optional\nfrom collections import Counter\nimport os\nimport csv","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:37:26.461903Z","iopub.execute_input":"2022-03-08T21:37:26.462258Z","iopub.status.idle":"2022-03-08T21:37:26.467682Z","shell.execute_reply.started":"2022-03-08T21:37:26.462208Z","shell.execute_reply":"2022-03-08T21:37:26.466725Z"},"trusted":true},"execution_count":188,"outputs":[]},{"cell_type":"markdown","source":"## Todo Part 1\nComplete the implementation of the encode method of the Tokenizer class:\n\n`encode`: encode a given space-separated text into list of token ids according to the `self.token2idx` property. For tokens not present in the mapping, use the id of the `<unk>` token. If `max_length` is set, pad the input to `max_length` if it is less than `max_length` and truncate to `max_length` if it exceeds the length.\n\nExamples\n```python\ntext = \"hello transformers !\"\ntokenizer.encode(text)                  # example output: [3, 4, 5]\ntokenizer.encode(text, max_length=5)    # example output: [3, 4, 5, 0, 0]\ntokenizer.encode(text, max_length=2)    # example output: [3, 4]\n```","metadata":{}},{"cell_type":"code","source":"class Tokenizer:\n    def __init__(self):\n        # two special tokens for padding and unknown\n        self.token2idx = {\"<pad>\": 0, \"<unk>\": 1}\n        self.idx2token = [\"<pad>\", \"<unk>\"]\n        self.is_fit = False\n    \n    @property\n    def pad_id(self):\n        return self.token2idx[\"<pad>\"]\n    \n    def __len__(self):\n        return len(self.idx2token)\n    \n    def fit(self, train_texts: List[str]):\n        counter = Counter()\n        for text in train_texts:\n            counter.update(text.lower().split())\n        \n        # manually set a vocabulary size for the data set\n        vocab_size = 20000\n        self.idx2token.extend([token for token, count in counter.most_common(vocab_size - 2)])\n        for (i, token) in enumerate(self.idx2token):\n            self.token2idx[token] = i\n            \n        self.is_fit = True\n                \n    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:\n        if not self.is_fit:\n            raise Exception(\"Please fit the tokenizer on the training tokens\")\n            \n        # TODO: implement the encode method, the method signature shouldn't be changed\n        text = text.lower()\n        tokens = text.split()\n        if max_length == None:\n            token_ids = [self.token2idx.get(token, self.token2idx['<unk>']) for token in tokens]\n            return token_ids\n        if max_length >= len(tokens):\n            token_ids = [self.token2idx.get(token, self.token2idx['<unk>']) for token in tokens] + [self.token2idx['<pad>']] * (max_length - len(tokens))\n            return token_ids\n        if max_length < len(tokens):\n            tokens = tokens[:max_length]\n            token_ids = [self.token2idx.get(token, self.token2idx['<unk>']) for token in tokens]\n            return token_ids\n        raise NotImplemented\n","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:37:26.470106Z","iopub.execute_input":"2022-03-08T21:37:26.470677Z","iopub.status.idle":"2022-03-08T21:37:26.484477Z","shell.execute_reply.started":"2022-03-08T21:37:26.470617Z","shell.execute_reply":"2022-03-08T21:37:26.483780Z"},"trusted":true},"execution_count":189,"outputs":[]},{"cell_type":"code","source":"def load_raw_data(filepath: str, with_tags: bool = True):\n    data = {'text': []}\n    if with_tags:\n        data['tags'] = []\n        with open(filepath) as f:\n            reader = csv.reader(f)\n            for text, tags in reader:\n                data['text'].append(text)\n                data['tags'].append(tags)\n    else:\n        with open(filepath) as f:\n            for line in f:\n                data['text'].append(line.strip())\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:37:26.485416Z","iopub.execute_input":"2022-03-08T21:37:26.485596Z","iopub.status.idle":"2022-03-08T21:37:26.496401Z","shell.execute_reply.started":"2022-03-08T21:37:26.485575Z","shell.execute_reply":"2022-03-08T21:37:26.495632Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/cs165b-w22-hw4/\"","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:37:26.497790Z","iopub.execute_input":"2022-03-08T21:37:26.498255Z","iopub.status.idle":"2022-03-08T21:37:26.507743Z","shell.execute_reply.started":"2022-03-08T21:37:26.498219Z","shell.execute_reply":"2022-03-08T21:37:26.507000Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntrain_raw = load_raw_data(os.path.join(data_dir, \"train.csv\"))\nval_raw = load_raw_data(os.path.join(data_dir, \"val.csv\"))\ntest_raw = load_raw_data(os.path.join(data_dir, \"test_tokens.txt\"), with_tags=False)\n# fit the tokenizer on the training tokens\ntokenizer.fit(train_raw['text'])","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:37:26.511093Z","iopub.execute_input":"2022-03-08T21:37:26.511366Z","iopub.status.idle":"2022-03-08T21:37:26.684055Z","shell.execute_reply.started":"2022-03-08T21:37:26.511335Z","shell.execute_reply":"2022-03-08T21:37:26.683373Z"},"trusted":true},"execution_count":192,"outputs":[]},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:37:26.685334Z","iopub.execute_input":"2022-03-08T21:37:26.685586Z","iopub.status.idle":"2022-03-08T21:37:26.690062Z","shell.execute_reply.started":"2022-03-08T21:37:26.685552Z","shell.execute_reply":"2022-03-08T21:37:26.689372Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"code","source":"class NERDataset:\n    tag2idx = {'O': 1, 'B-PER': 2, 'I-PER': 3, 'B-ORG': 4, 'I-ORG': 5, 'B-LOC': 6, 'I-LOC': 7, 'B-MISC': 8, 'I-MISC': 9}\n    idx2tag = ['<pad>', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG','B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n    \n    def __init__(self, raw_data: Dict[str, List[str]], tokenizer: Tokenizer, max_length: int = 128):\n        self.tokenizer = tokenizer\n        self.token_ids = []\n        self.tag_ids = []\n        self.with_tags = False\n        for text in raw_data['text']:\n            self.token_ids.append(tokenizer.encode(text, max_length=max_length))\n        if 'tags' in raw_data:\n            self.with_tags = True\n            for tags in raw_data['tags']:\n                self.tag_ids.append(self.encode_tags(tags, max_length=max_length))\n    \n    def encode_tags(self, tags: str, max_length: Optional[int] = None):\n        tag_ids = [self.tag2idx[tag] for tag in tags.split()]\n        if max_length is None:\n            return tag_ids\n        # truncate the tags if longer than max_length\n        if len(tag_ids) > max_length:\n            return tag_ids[:max_length]\n        # pad with 0s if shorter than max_length\n        else:\n            return tag_ids + [0] * (max_length - len(tag_ids))  # 0 as padding for tags\n        \n    def __len__(self):\n        return len(self.token_ids)\n    \n    def __getitem__(self, idx):\n        token_ids = torch.LongTensor(self.token_ids[idx])\n        mask = token_ids == self.tokenizer.pad_id  # padding tokens\n        if self.with_tags:\n            # for training and validation\n            return token_ids, mask, torch.LongTensor(self.tag_ids[idx])\n        else:\n            # for testing\n            return token_ids, mask\n        ","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:37:26.691583Z","iopub.execute_input":"2022-03-08T21:37:26.692117Z","iopub.status.idle":"2022-03-08T21:37:26.705696Z","shell.execute_reply.started":"2022-03-08T21:37:26.692079Z","shell.execute_reply":"2022-03-08T21:37:26.704926Z"},"trusted":true},"execution_count":194,"outputs":[]},{"cell_type":"code","source":"tr_data = NERDataset(train_raw, tokenizer)\nva_data = NERDataset(val_raw, tokenizer)\nte_data = NERDataset(test_raw, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:37:26.707283Z","iopub.execute_input":"2022-03-08T21:37:26.707575Z","iopub.status.idle":"2022-03-08T21:37:27.055510Z","shell.execute_reply.started":"2022-03-08T21:37:26.707523Z","shell.execute_reply":"2022-03-08T21:37:27.054352Z"},"trusted":true},"execution_count":195,"outputs":[]},{"cell_type":"markdown","source":"## Todo Part 2\nImplement and experiment with transformer models. The implementation should include **at least** the following:\n- `nn.Embedding` layer to embed input token ids to the embedding space\n- `nn.TransformerEncoder` layer to perform transformer operations\n- `nn.Linear` layer as the output layer to map the output to the number of classes\n\nAs we will be using the cross-entropy loss, an `nn.Softmax` or `nn.LogSoftmax` layer is not needed.\n\nYou can refer to the following links for transformer Docs and examples:\n\nhttps://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html\n\nhttps://pytorch.org/tutorials/beginner/transformer_tutorial.html\n\nYou can modify the `__init__` method including the signature needed. For the `forward` method, the method signature is given as follows:\n\n- `src`: a `torch.LongTensor` of shape (batch_size, max_length, vocab_size) representing the input text tokens.\n\n- `src_mask`: a `torch.BoolTensor` of shape (batch_size, max_length) indicating whether an input position is padded. This is needed to prevent the transformer model attending to padded tokens.\n\nThe output from the `forward` method should be of shape (batch_size, max_length, num_classes). Note that the number of classes should be 10 instead of 9 because of an additional padding class.\n","metadata":{}},{"cell_type":"code","source":"# TODO: implement the Transformer model architecture and forward method\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 128):#5000\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n                 nlayers: int, dropout: float = 0.15):#0.15\n        super().__init__()\n        self.model_type = 'Transformer'\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, d_model)\n        self.d_model = d_model\n        self.decoder = nn.Linear(d_model, ntoken)\n        self.fc1 = nn.Linear(ntoken, 10)\n\n        self.init_weights()\n\n    def init_weights(self) -> None:\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            src: Tensor, shape [seq_len, batch_size]\n            src_mask: Tensor, shape [seq_len, seq_len]\n\n        Returns:\n            output Tensor of shape [seq_len, batch_size, ntoken]\n        \"\"\"\n        src = self.encoder(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_key_padding_mask = src_mask)\n        output = self.decoder(output)\n        output = self.fc1(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:37:27.056895Z","iopub.execute_input":"2022-03-08T21:37:27.057278Z","iopub.status.idle":"2022-03-08T21:37:27.083583Z","shell.execute_reply.started":"2022-03-08T21:37:27.057231Z","shell.execute_reply":"2022-03-08T21:37:27.082771Z"},"trusted":true},"execution_count":196,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nimport torchmetrics\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:37:27.084864Z","iopub.execute_input":"2022-03-08T21:37:27.085274Z","iopub.status.idle":"2022-03-08T21:37:27.095430Z","shell.execute_reply.started":"2022-03-08T21:37:27.085233Z","shell.execute_reply":"2022-03-08T21:37:27.094600Z"},"trusted":true},"execution_count":197,"outputs":[]},{"cell_type":"code","source":"def validate(\n    model: nn.Module, \n    dataloader: DataLoader, \n    device: torch.device,\n):\n    acc_metric = torchmetrics.Accuracy(compute_on_step=False).to(device)\n    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n    model.eval()\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader):\n            input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n            # output shape: (batch_size, max_length, num_classes)\n            logits = model(input_ids, input_mask)\n            # ignore padding index 0 when calculating loss\n            loss = F.cross_entropy(logits.view(-1, 10), tags.view(-1), ignore_index=0)\n                \n            loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n            is_active = torch.logical_not(input_mask)  # non-padding elements\n            # only consider non-padded tokens when calculating accuracy\n            acc_metric.update(logits[is_active], tags[is_active])\n            \n    print(f\"| Validate | loss {loss_metric.compute()[0]:.4f} | acc {acc_metric.compute():.4f} |\")","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:37:27.096719Z","iopub.execute_input":"2022-03-08T21:37:27.097405Z","iopub.status.idle":"2022-03-08T21:37:27.113851Z","shell.execute_reply.started":"2022-03-08T21:37:27.097368Z","shell.execute_reply":"2022-03-08T21:37:27.112968Z"},"trusted":true},"execution_count":198,"outputs":[]},{"cell_type":"code","source":"def train(\n    model: nn.Module, \n    dataloader: DataLoader, \n    optimizer: optim.Optimizer,\n    device: torch.device,\n    epoch: int,\n):\n    acc_metric = torchmetrics.Accuracy(compute_on_step=False).to(device)\n    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n    model.train()\n    \n    # loop through all batches in the training\n    for batch in tqdm(dataloader):\n        input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n        optimizer.zero_grad()\n        # output shape: (batch_size, max_length, num_classes)\n        logits = model(input_ids, input_mask)\n        # ignore padding index 0 when calculating loss\n        loss = F.cross_entropy(logits.view(-1, 10), tags.view(-1), ignore_index=0)\n        \n        loss.backward()\n        optimizer.step()\n        \n        loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n        is_active = torch.logical_not(input_mask)  # non-padding elements\n        # only consider non-padded tokens when calculating accuracy\n        acc_metric.update(logits[is_active], tags[is_active])\n    \n    print(f\"| Epoch {epoch} | loss {loss_metric.compute()[0]:.4f} | acc {acc_metric.compute():.4f} |\")\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:37:27.115116Z","iopub.execute_input":"2022-03-08T21:37:27.115555Z","iopub.status.idle":"2022-03-08T21:37:27.129510Z","shell.execute_reply.started":"2022-03-08T21:37:27.115520Z","shell.execute_reply":"2022-03-08T21:37:27.128713Z"},"trusted":true},"execution_count":199,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# data loaders\ntrain_dataloader = DataLoader(tr_data, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(va_data, batch_size=32)\ntest_dataloader = DataLoader(te_data, batch_size=32)\n\n# move the model to device\nmodel = TransformerModel(\n    ntoken = len(tokenizer), \n    d_model = 256, \n    nhead = 4, \n    d_hid = 256, \n    nlayers = 2).to(device)\noptimizer = optim.Adam(model.parameters())\n\nfor epoch in range(10):\n    if epoch > 3:\n        optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n    if epoch > 6:\n        optimizer = optim.Adam(model.parameters(), lr = 0.00001)\n    train(model, train_dataloader, optimizer, device, epoch)\n    validate(model, val_dataloader, device)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:44:02.391209Z","iopub.execute_input":"2022-03-08T21:44:02.391509Z","iopub.status.idle":"2022-03-08T21:47:04.353464Z","shell.execute_reply.started":"2022-03-08T21:44:02.391478Z","shell.execute_reply":"2022-03-08T21:47:04.352774Z"},"trusted":true},"execution_count":202,"outputs":[]},{"cell_type":"markdown","source":"## Todo Part 3\nMake predictions on the validation data and evaluate entity-level F1 scores using conlleval script.\n\n`predict`: taking inputs of a trained model, a dataloader, and a torch device, predict the tags for all tokens in the data set. The output should be a nested list of lists, each containing tag predictions for a single sentence.\n\n    Input texts in the dataloader (2 sentences):\n    EU rejects German call\n    Only France and Britain backed Fischler 's proposal .\n    \n    Example output:\n    [['B-ORG', 'O', 'B-MISC', 'O'], ['O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O']]\n        ","metadata":{}},{"cell_type":"code","source":"# TODO: implement the predict function\ndef predict(model: nn.Module, dataloader: DataLoader, device: torch.device) -> List[List[str]]:\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for batch in tqdm(dataloader):\n            input_ids, input_mask= batch[0].to(device), batch[1].to(device)\n            output = model(input_ids, input_mask)\n            predicted = output.argmax(axis=-1)\n            for i in range(0, len(predicted)):\n                predict = []\n                for j in range(0, len(predicted[i])):\n                    if int(batch[0][i][j]) == 0:\n                        continue\n                    else:\n                        predict.append(NERDataset.idx2tag[predicted[i][j]])\n                preds.append(predict)\n            #raise NotImplemented\n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:47:23.118942Z","iopub.execute_input":"2022-03-08T21:47:23.119520Z","iopub.status.idle":"2022-03-08T21:47:23.126804Z","shell.execute_reply.started":"2022-03-08T21:47:23.119480Z","shell.execute_reply":"2022-03-08T21:47:23.126128Z"},"trusted":true},"execution_count":203,"outputs":[]},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\nfrom conlleval import evaluate","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:47:26.318464Z","iopub.execute_input":"2022-03-08T21:47:26.319016Z","iopub.status.idle":"2022-03-08T21:47:27.265017Z","shell.execute_reply.started":"2022-03-08T21:47:26.318977Z","shell.execute_reply":"2022-03-08T21:47:27.264206Z"},"trusted":true},"execution_count":204,"outputs":[]},{"cell_type":"code","source":"# YOU SHOULD NOT CHANGE THIS CODEBLOCK\n# use the conlleval script to measure the entity-level f1\npred_tags = []\nfor tags in predict(model, val_dataloader, device):\n    pred_tags.extend(tags)\n    pred_tags.append('O')\n    \ntrue_tags = []\nfor tags in val_raw['tags']:\n    true_tags.extend(tags.strip().split())\n    true_tags.append('O')\n\nevaluate(true_tags, pred_tags)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:47:29.674550Z","iopub.execute_input":"2022-03-08T21:47:29.675345Z","iopub.status.idle":"2022-03-08T21:47:33.443140Z","shell.execute_reply.started":"2022-03-08T21:47:29.675301Z","shell.execute_reply":"2022-03-08T21:47:33.442435Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"markdown","source":"Example output from the above codeblock. We will take the overall test F1 score (69.24 in this example) and grade accordingly.\n```\nprocessed 54612 tokens with 5942 phrases; found: 5554 phrases; correct: 3980.\naccuracy:  65.78%; (non-O)\naccuracy:  93.88%; precision:  71.66%; recall:  66.98%; FB1:  69.24\n              LOC: precision:  84.58%; recall:  77.03%; FB1:  80.63  1673\n             MISC: precision:  77.31%; recall:  71.69%; FB1:  74.40  855\n              ORG: precision:  58.71%; recall:  63.83%; FB1:  61.16  1458\n              PER: precision:  66.84%; recall:  56.89%; FB1:  61.47  1568\n(71.66006481814908, 66.98081454055873, 69.24147529575504)\n```\nIf the codeblock above errors out, check your implementation of the `predict` function. It should return a nested list of lists, each containing predicted tags in their IOB string forms.","metadata":{}},{"cell_type":"markdown","source":"## Todo Part 4\nOnce you finish all previous todos and are satisfied with the model performance on the validation set, make predictions on the test set and keep a copy of the `submission.txt` file by downloading it to your local machine. You can find `submission.txt` under Output > `/kaggle/working`.","metadata":{}},{"cell_type":"code","source":"# YOU SHOULD NOT CHANGE THIS CODEBLOCK\n# make prediction on the test set and save to submission.txt\npreds = predict(model, test_dataloader, device)\nwith open(\"submission.txt\", \"w\") as f:\n    for tags in preds:\n        f.write(\" \".join(tags) + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:47:53.998614Z","iopub.execute_input":"2022-03-08T21:47:53.999073Z","iopub.status.idle":"2022-03-08T21:47:57.716006Z","shell.execute_reply.started":"2022-03-08T21:47:53.999022Z","shell.execute_reply":"2022-03-08T21:47:57.715186Z"},"trusted":true},"execution_count":206,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:48:01.749454Z","iopub.execute_input":"2022-03-08T21:48:01.749732Z","iopub.status.idle":"2022-03-08T21:48:01.755321Z","shell.execute_reply.started":"2022-03-08T21:48:01.749700Z","shell.execute_reply":"2022-03-08T21:48:01.754620Z"},"trusted":true},"execution_count":207,"outputs":[]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:48:03.753064Z","iopub.execute_input":"2022-03-08T21:48:03.753605Z","iopub.status.idle":"2022-03-08T21:48:04.446994Z","shell.execute_reply.started":"2022-03-08T21:48:03.753563Z","shell.execute_reply":"2022-03-08T21:48:04.446193Z"},"trusted":true},"execution_count":208,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}